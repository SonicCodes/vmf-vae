{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "bs = 256\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import special as sp\n",
    "\n",
    "\n",
    "def GVar(x):\n",
    "    return x.to(\"cuda\")  # pytorch 0.4.1\n",
    "\n",
    "class vMF(torch.nn.Module):\n",
    "    def __init__(self, hid_dim, lat_dim, kappa=1):\n",
    "        \"\"\"\n",
    "        von Mises-Fisher distribution class with batch support and manual tuning kappa value.\n",
    "        Implementation follows description of my paper and Guu's.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.lat_dim = lat_dim\n",
    "        self.kappa = kappa\n",
    "        # self.func_kappa = torch.nn.Linear(hid_dim, lat_dim)\n",
    "        self.func_mu = torch.nn.Linear(hid_dim, lat_dim)\n",
    "\n",
    "        self.kld = GVar(torch.from_numpy(vMF._vmf_kld(kappa, lat_dim)).float())\n",
    "        print('KLD: {}'.format(self.kld.data[0]))\n",
    "\n",
    "    def estimate_param(self, latent_code):\n",
    "        ret_dict = {}\n",
    "        ret_dict['kappa'] = self.kappa\n",
    "\n",
    "        # Only compute mu, use mu/mu_norm as mu,\n",
    "        #  use 1 as norm, use diff(mu_norm, 1) as redundant_norm\n",
    "        mu = self.func_mu(latent_code)\n",
    "\n",
    "        norm = torch.norm(mu, 2, 1, keepdim=True)\n",
    "        mu_norm_sq_diff_from_one = torch.pow(torch.add(norm, -1), 2)\n",
    "        redundant_norm = torch.sum(mu_norm_sq_diff_from_one, dim=1, keepdim=True)\n",
    "        ret_dict['norm'] = torch.ones_like(mu)\n",
    "        ret_dict['redundant_norm'] = redundant_norm\n",
    "\n",
    "        mu = mu / torch.norm(mu, p=2, dim=1, keepdim=True)\n",
    "        ret_dict['mu'] = mu\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "    def compute_KLD(self, tup, batch_sz):\n",
    "        return self.kld.expand(batch_sz)\n",
    "\n",
    "    @staticmethod\n",
    "    def _vmf_kld(k, d):\n",
    "        tmp = (k * ((sp.iv(d / 2.0 + 1.0, k) + sp.iv(d / 2.0, k) * d / (2.0 * k)) / sp.iv(d / 2.0, k) - d / (2.0 * k)) \\\n",
    "               + d * np.log(k) / 2.0 - np.log(sp.iv(d / 2.0, k)) \\\n",
    "               - sp.loggamma(d / 2 + 1) - d * np.log(2) / 2).real\n",
    "        if tmp != tmp:\n",
    "            exit()\n",
    "        return np.array([tmp])\n",
    "\n",
    "    @staticmethod\n",
    "    def _vmf_kld_davidson(k, d):\n",
    "        \"\"\"\n",
    "        This should be the correct KLD.\n",
    "        Empirically we find that _vmf_kld (as in the Guu paper) only deviates a little (<2%) in most cases we use.\n",
    "        \"\"\"\n",
    "        tmp = k * sp.iv(d / 2, k) / sp.iv(d / 2 - 1, k) + (d / 2 - 1) * torch.log(k) - torch.log(\n",
    "            sp.iv(d / 2 - 1, k)) + np.log(np.pi) * d / 2 + np.log(2) - sp.loggamma(d / 2).real - (d / 2) * np.log(\n",
    "            2 * np.pi)\n",
    "        if tmp != tmp:\n",
    "            exit()\n",
    "        return np.array([tmp])\n",
    "\n",
    "    def build_bow_rep(self, lat_code, n_sample):\n",
    "        batch_sz = lat_code.size()[0]\n",
    "        tup = self.estimate_param(latent_code=lat_code)\n",
    "        mu = tup['mu']\n",
    "        norm = tup['norm']\n",
    "        kappa = tup['kappa']\n",
    "\n",
    "        kld = self.compute_KLD(tup, batch_sz)\n",
    "        vecs = []\n",
    "        if n_sample == 1:\n",
    "            return tup, kld, self.sample_cell(mu, norm, kappa)\n",
    "        for n in range(n_sample):\n",
    "            sample = self.sample_cell(mu, norm, kappa)\n",
    "            vecs.append(sample)\n",
    "        vecs = torch.cat(vecs, dim=0)\n",
    "        return tup, kld, vecs\n",
    "\n",
    "    def sample_cell(self, mu, norm, kappa):\n",
    "        batch_sz, lat_dim = mu.size()\n",
    "        # mu = GVar(mu)\n",
    "        mu = mu / torch.norm(mu, p=2, dim=1, keepdim=True)\n",
    "        w = self._sample_weight_batch(kappa, lat_dim, batch_sz)\n",
    "        w = w.unsqueeze(1)\n",
    "\n",
    "        # batch version\n",
    "        w_var = GVar(w * torch.ones(batch_sz, lat_dim))\n",
    "        v = self._sample_ortho_batch(mu, lat_dim)\n",
    "        scale_factr = torch.sqrt(\n",
    "            GVar(torch.ones(batch_sz, lat_dim)) - torch.pow(w_var, 2))\n",
    "        orth_term = v * scale_factr\n",
    "        muscale = mu * w_var\n",
    "        sampled_vec = orth_term + muscale\n",
    "\n",
    "        return sampled_vec.unsqueeze(0)\n",
    "\n",
    "    def _sample_weight_batch(self, kappa, dim, batch_sz=1):\n",
    "        result = torch.FloatTensor((batch_sz))\n",
    "        for b in range(batch_sz):\n",
    "            result[b] = self._sample_weight(kappa, dim)\n",
    "        return result\n",
    "\n",
    "    def _sample_weight(self, kappa, dim):\n",
    "        \"\"\"Rejection sampling scheme for sampling distance from center on\n",
    "        surface of the sphere.\n",
    "        \"\"\"\n",
    "        dim = dim - 1  # since S^{n-1}\n",
    "        b = dim / (np.sqrt(4. * kappa ** 2 + dim ** 2) + 2 * kappa)  # b= 1/(sqrt(4.* kdiv**2 + 1) + 2 * kdiv)\n",
    "        x = (1. - b) / (1. + b)\n",
    "        c = kappa * x + dim * np.log(1 - x ** 2)  # dim * (kdiv *x + np.log(1-x**2))\n",
    "\n",
    "        while True:\n",
    "            z = np.random.beta(dim / 2., dim / 2.)  # concentrates towards 0.5 as d-> inf\n",
    "            w = (1. - (1. + b) * z) / (1. - (1. - b) * z)\n",
    "            u = np.random.uniform(low=0, high=1)\n",
    "            if kappa * w + dim * np.log(1. - x * w) - c >= np.log(\n",
    "                    u):  # thresh is dim *(kdiv * (w-x) + log(1-x*w) -log(1-x**2))\n",
    "                return w\n",
    "\n",
    "    def _sample_ortho_batch(self, mu, dim):\n",
    "        \"\"\"\n",
    "\n",
    "        :param mu: Variable, [batch size, latent dim]\n",
    "        :param dim: scala. =latent dim\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        _batch_sz, _lat_dim = mu.size()\n",
    "        assert _lat_dim == dim\n",
    "        squeezed_mu = mu.unsqueeze(1)\n",
    "\n",
    "        v = GVar(torch.randn(_batch_sz, dim, 1))  # TODO random\n",
    "\n",
    "        # v = GVar(torch.linspace(-1, 1, steps=dim))\n",
    "        # v = v.expand(_batch_sz, dim).unsqueeze(2)\n",
    "\n",
    "        rescale_val = torch.bmm(squeezed_mu, v).squeeze(2)\n",
    "        proj_mu_v = mu * rescale_val\n",
    "        ortho = v.squeeze() - proj_mu_v\n",
    "        ortho_norm = torch.norm(ortho, p=2, dim=1, keepdim=True)\n",
    "        y = ortho / ortho_norm\n",
    "        return y\n",
    "\n",
    "    def _sample_orthonormal_to(self, mu, dim):\n",
    "        \"\"\"Sample point on sphere orthogonal to mu.\n",
    "        \"\"\"\n",
    "        v = GVar(torch.randn(dim))  # TODO random\n",
    "\n",
    "        # v = GVar(torch.linspace(-1,1,steps=dim))\n",
    "\n",
    "        rescale_value = mu.dot(v) / mu.norm()\n",
    "        proj_mu_v = mu * rescale_value.expand(dim)\n",
    "        ortho = v - proj_mu_v\n",
    "        ortho_norm = torch.norm(ortho)\n",
    "        return ortho / ortho_norm.expand_as(ortho)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Gauss(nn.Module):\n",
    "    # __slots__ = ['lat_dim', 'logvar', 'mean']\n",
    "\n",
    "    def __init__(self, hid_dim, lat_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.lat_dim = lat_dim\n",
    "        self.func_mean = torch.nn.Linear(hid_dim, lat_dim)\n",
    "        self.func_logvar = torch.nn.Linear(hid_dim, lat_dim)\n",
    "\n",
    "    def estimate_param(self, latent_code):\n",
    "        mean = self.func_mean(latent_code)\n",
    "        logvar = self.func_logvar(latent_code)\n",
    "        return {'mean': mean, 'logvar': logvar}\n",
    "\n",
    "    def compute_KLD(self, tup):\n",
    "        mean = tup['mean']\n",
    "        logvar = tup['logvar']\n",
    "\n",
    "        kld = -0.5 * torch.sum(1 - torch.mul(mean, mean) +\n",
    "                               2 * logvar - torch.exp(2 * logvar), dim=1)\n",
    "        return kld\n",
    "\n",
    "    def sample_cell(self, batch_size):\n",
    "        eps = torch.autograd.Variable(torch.normal(torch.zeros((batch_size, self.lat_dim))))\n",
    "        eps.to(device)\n",
    "        return eps.unsqueeze(0)\n",
    "\n",
    "    def build_bow_rep(self, lat_code, n_sample):\n",
    "        batch_sz = lat_code.size()[0]\n",
    "        tup = self.estimate_param(latent_code=lat_code)\n",
    "        mean = tup['mean']\n",
    "        logvar = tup['logvar']\n",
    "\n",
    "        kld = self.compute_KLD(tup)\n",
    "        if n_sample == 1:\n",
    "            eps = self.sample_cell(batch_size=batch_sz).to(device)\n",
    "            vec = torch.mul(torch.exp(logvar), eps) + mean\n",
    "            return tup, kld, vec\n",
    "\n",
    "        vecs = []\n",
    "        for ns in range(n_sample):\n",
    "            eps = self.sample_cell(batch_size=batch_sz)\n",
    "            vec = torch.mul(torch.exp(logvar), eps) + mean\n",
    "            vecs.append(vec)\n",
    "        vecs = torch.cat(vecs, dim=0)\n",
    "        return tup, kld, vecs\n",
    "\n",
    "    def get_aux_loss_term(self, tup):\n",
    "\n",
    "        return torch.from_numpy(np.zeros([1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, prior=\"gauss\", hparam=None):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        self.prior = prior\n",
    "        if prior == \"vmf\":\n",
    "            self.hparam = 1 if hparam is None else hparam\n",
    "            self.dist = vMF(z_dim*2, z_dim, kappa=self.hparam)\n",
    "            \n",
    "        elif prior == \"gauss\":\n",
    "            self.dist = Gauss(z_dim*2, z_dim)\n",
    "            self.hparam = None\n",
    "        \n",
    "        self.sampled_z = None\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    def div_loss(self, z1, z2):\n",
    "        linear_x_2 = torch.cat([z1, z2], dim=-1)\n",
    "        tup = self.dist.estimate_param(latent_code=linear_x_2)\n",
    "        batch_sz = z1.size()[0]\n",
    "        if self.prior == \"gauss\":\n",
    "            kld = self.dist.compute_KLD(tup)\n",
    "        else:\n",
    "            kld = self.dist.compute_KLD(tup, batch_sz)\n",
    "        return kld.sum()\n",
    "    def sampling(self, mu, log_var):\n",
    "        linear_x_2 = torch.cat([mu, log_var], dim=-1)\n",
    "        tup, _, vecs = self.dist.build_bow_rep(linear_x_2, 1)\n",
    "        batch_sz = mu.size()[0]\n",
    "        return vecs.view(batch_sz, -1)\n",
    "    \n",
    "    def log_latents(self, z1, z2):\n",
    "        linear_x_2 = torch.cat([z1, z2], dim=-1)\n",
    "        # matplot to show the distribution of the latent space\n",
    "        # sub axis, 2 rows  , 2 columns\n",
    "        fig, axs = plt.subplots(2, 2)\n",
    "        axs[0, 0].hist(z1.cpu().detach().numpy())\n",
    "        axs[0, 0].set_title('z1 histogram')\n",
    "        axs[0, 1].hist(z2.cpu().detach().numpy())\n",
    "        axs[0, 1].set_title('z2 histogram')\n",
    "        axs[1, 0].plot(self.sampled_z.cpu().detach().numpy())\n",
    "        axs[1, 0].set_title('sampled_z')\n",
    "        axs[1, 1].hist(self.sampled_z.flatten().cpu().detach().numpy())\n",
    "        axs[1, 1].set_title('sampled z histogram')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(\"./samples/latent_\"+self.prior+ f\"_{self.hparam or ''}\"+\".png\") \n",
    "        plt.close()\n",
    "            # return \n",
    "\n",
    "    def generate_sample(self):\n",
    "        if self.prior == \"gauss\":\n",
    "            z = torch.randn(64, 4).cuda()\n",
    "            sample = vae.decoder(z).cuda()\n",
    "            \n",
    "            save_image(sample.view(64, 1, 28, 28), './samples/sample_' + self.prior + f\"_{self.hparam or ''}\" + '.png')\n",
    "            print(\"sample generated\")\n",
    "        elif self.prior == \"vmf\":\n",
    "            mu = torch.rand(25, 4).cuda()\n",
    "            mu = mu / torch.norm(mu, p=2, dim=1, keepdim=True)\n",
    "            images = []\n",
    "            for i in torch.linspace(0.1, 15, steps=5):\n",
    "                # i = i.round()\n",
    "                z = self.dist.sample_cell(mu, None, i)\n",
    "                sample = vae.decoder(z).cuda()\n",
    "\n",
    "                grid = make_grid(sample.view(25, 1, 28, 28), nrow=5)\n",
    "                # Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\n",
    "                ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "                _im = Image.fromarray(ndarr)\n",
    "                im = Image.new('RGB', (_im.width, _im.height+40), (255, 255, 255))\n",
    "                im.paste(_im, (0, 0))\n",
    "                draw = ImageDraw.Draw(im)\n",
    "                # font with size 30px\n",
    "                font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", size=20)\n",
    "                draw.text((\n",
    "                    20 , _im.height + 10), f\"Kappa: {i:.2f}\", (0, 0, 0), font)\n",
    "                    # , _im.height), f\"Kappa: {i}\", (0, 0, 0))\n",
    "                # im.save(fp, format=format)\n",
    "                images.append(im)\n",
    "            image_draw = Image.new('RGB', (\n",
    "                len(images) * (images[0].size[0] + 10), images[0].size[1]), (255, 255, 255))\n",
    "                # , 280))\n",
    "            for i, image in enumerate(images):\n",
    "                image_draw.paste(image, (i * (images[0].size[0] + 10), 0))\n",
    "            image_draw.save('./samples/sample_' + self.prior + f\"_{self.hparam or ''}\" + '.png')\n",
    "            # z = self.dist.sample_cell(mu, None, self.hparam) # kappa is adjustable here\n",
    "            \n",
    "        \n",
    "\n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1, z2 = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(z1, z2)\n",
    "        self.sampled_z = z\n",
    "        return self.decoder(z),  z1, z2\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=4, prior=\"vmf\", hparam=50)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD =  vae.div_loss(mu, log_var)\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        vae.log_latents(mu, log_var)\n",
    "        vae.generate_sample()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
